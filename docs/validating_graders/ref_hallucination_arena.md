# Validate on Reference Hallucination Arena

Evaluate how accurately LLMs recommend real academic references. This benchmark verifies every paper citation against four authoritative academic databases—Crossref, PubMed, arXiv, and DBLP—providing objective, reproducible metrics to measure reference hallucination across models and disciplines.


## What is Reference Hallucination Arena?

Reference Hallucination Arena is a benchmark designed to evaluate LLMs' ability to recommend **real, verifiable academic papers**. Unlike subjective evaluation tasks, this benchmark uses fully automated, objective verification: every reference generated by a model is checked against real-world academic databases.

The benchmark addresses a critical problem: when researchers ask LLMs for literature recommendations, models frequently "hallucinate" references—generating papers that sound plausible but do not actually exist. Reference Hallucination Arena quantifies this phenomenon across multiple models and academic disciplines.

The official evaluation dataset is available on HuggingFace: [**agentscope-ai/OpenJudge**](https://huggingface.co/datasets/agentscope-ai/OpenJudge/tree/main/benchmark). You can download `ref_hallucination_query.json` from the `benchmark/` directory.

**Key Features:**

| Feature | Description |
|---------|-------------|
| **Multi-source Verification** | Cross-validates references against Crossref, PubMed, arXiv, and DBLP |
| **Multi-discipline Coverage** | Supports Computer Science, Biomedical, Physics, Chemistry, Mathematics, and more |
| **Field-level Accuracy** | Checks title, author, year, and DOI individually for fine-grained analysis |
| **Year Constraint Support** | Tests whether models respect temporal constraints (e.g., "papers after 2020") |
| **Checkpoint Resume** | Fine-grained per-item checkpointing for long-running evaluations |
| **Objective Metrics** | No subjective judgment—all scores are based on verifiable facts |

The evaluation pipeline consists of six automated steps:

| Step | Component | Description |
|------|-----------|-------------|
| 1 | `DatasetLoader` | Load evaluation queries from JSON/JSONL dataset |
| 2 | `ResponseCollector` | Collect BibTeX-formatted responses from target models |
| 3 | `BibExtractor` | Extract structured references from model responses |
| 4 | `CompositeVerifier` | Verify each reference against Crossref/PubMed/arXiv/DBLP |
| 5 | `ObjectiveScorer` + `RankingCalculator` | Compute verification metrics and rank models |
| 6 | `RefReportGenerator` + `RefChartGenerator` | Generate detailed report and visualization charts |


## Dataset

The evaluation dataset is hosted on HuggingFace: [**agentscope-ai/OpenJudge**](https://huggingface.co/datasets/agentscope-ai/OpenJudge/tree/main/benchmark). Download `ref_hallucination_query.json` from the `benchmark/` directory to use as your query file.

Each query item in the dataset follows this schema:

```json
{
  "query": "Please recommend papers on Transformer architectures for NLP.",
  "discipline": "computer_science",
  "num_refs": 5,
  "language": "en",
  "year_constraint": {"min_year": 2020}
}
```

| Field | Required | Description |
|-------|----------|-------------|
| `query` | Yes | The prompt text for reference recommendation |
| `discipline` | No | Academic discipline for verification routing |
| `num_refs` | No | Expected number of references (default: 5) |
| `language` | No | Query language: `zh` or `en` (default: `zh`) |
| `year_constraint` | No | Time constraint on recommended references |

**Year Constraint Formats:**

| Format | Example | Meaning |
|--------|---------|---------|
| Exact year | `{"exact": 2023}` | Only papers from 2023 |
| Year range | `{"min_year": 2020, "max_year": 2024}` | Papers between 2020–2024 |
| After a year | `{"min_year": 2020}` | Papers from 2020 onwards |
| Before a year | `{"max_year": 2015}` | Papers before 2015 |

You can download the dataset and use it directly, or create your own custom queries following the same format.


## How to Run the Evaluation

Follow this workflow to evaluate your models' reference recommendation capabilities:

<div class="workflow-single">
<div class="workflow-header">Evaluation Workflow</div>

<div class="workflow">
<ol class="workflow-steps">
<li><strong>Prepare Dataset</strong>

Download the official dataset from HuggingFace or create your own query file in JSON/JSONL format.

???+ example "Show Code"
    ```bash
    # Option 1: Use the bundled example queries
    ls cookbooks/ref_hallucination_arena/examples/queries_example.json

    # Option 2: Download from HuggingFace
    pip install huggingface_hub
    python -c "
    from huggingface_hub import hf_hub_download
    hf_hub_download(
        repo_id='agentscope-ai/OpenJudge',
        filename='benchmark/ref_hallucination_query.json',
        repo_type='dataset',
        local_dir='./data'
    )
    "
    ```

    Or download directly from the [benchmark directory on HuggingFace](https://huggingface.co/datasets/agentscope-ai/OpenJudge/tree/main/benchmark).
</li>
<li><strong>Configure Endpoints</strong>

Create a YAML configuration file defining target models, verification settings, and output options.

???+ example "Show Code"
    ```yaml
    task:
      description: "Evaluate LLM reference recommendation capabilities"

    dataset:
      path: "./data/queries.json"

    target_endpoints:
      qwen:
        base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
        api_key: "${DASHSCOPE_API_KEY}"
        model: "qwen3-max"
        system_prompt: "You are an academic literature recommendation expert. Recommend {num_refs} real papers in BibTeX format."
        extra_params:
          temperature: 0.3

      gpt4:
        base_url: "https://api.openai.com/v1"
        api_key: "${OPENAI_API_KEY}"
        model: "gpt-4"
        system_prompt: "You are an academic literature recommendation expert. Recommend {num_refs} real papers in BibTeX format."
        extra_params:
          temperature: 0.3

    verification:
      max_workers: 10
      verified_threshold: 0.7

    output:
      output_dir: "./evaluation_results/ref_hallucination_arena"

    report:
      enabled: true
      language: "en"
    ```
</li>
<li><strong>Run Evaluation</strong>

Execute the pipeline via CLI or Python API. The pipeline supports checkpoint resume for long-running evaluations.

???+ example "Show Code"

    === "CLI"

        ```bash
        # Run evaluation with config file
        python -m cookbooks.ref_hallucination_arena --config config.yaml --save

        # Resume from checkpoint (default behavior)
        python -m cookbooks.ref_hallucination_arena --config config.yaml --save

        # Start fresh, ignore checkpoint
        python -m cookbooks.ref_hallucination_arena --config config.yaml --fresh --save
        ```

    === "Python API"

        ```python
        import asyncio
        from cookbooks.ref_hallucination_arena.pipeline import RefArenaPipeline

        async def main():
            pipeline = RefArenaPipeline.from_config("config.yaml")
            result = await pipeline.evaluate()

            # Print rankings
            for rank, (model, score) in enumerate(result.rankings, 1):
                print(f"{rank}. {model}: {score:.1%}")

        asyncio.run(main())
        ```
</li>
</ol>
</div>
</div>

!!! note "Environment Variables"
    Use `${ENV_VAR}` syntax in YAML config to reference environment variables for API keys. Never hardcode sensitive credentials in configuration files.


## Interpreting Results

The primary metrics are **verification rate** (percentage of references confirmed as real) and **hallucination rate** (percentage of fabricated references):

```
============================================================
REFERENCE HALLUCINATION ARENA RESULTS
============================================================
Model Rankings (by Verification Rate):
  1. Kimi             : 78.4% verified (hallucination: 21.6%)
  2. GPT-4            : 75.2% verified (hallucination: 24.8%)
  3. DeepSeek         : 72.8% verified (hallucination: 27.2%)
  4. Qwen             : 69.5% verified (hallucination: 30.5%)
============================================================
```

**Interpretation:**

- **> 75%** — Excellent: Model rarely hallucinates references
- **60-75%** — Good: Most references are real, but some fabrication occurs
- **40-60%** — Fair: Significant hallucination, use with caution
- **< 40%** — Poor: Model frequently fabricates references

Beyond overall rates, examine **per-field accuracy** for fine-grained insight:

```
Per-Field Accuracy (Qwen):
  Title Accuracy  : 82.3%    # Percentage of titles matching real papers
  Author Accuracy : 68.5%    # Percentage of correct author lists
  Year Accuracy   : 71.2%    # Percentage of correct publication years
  DOI Accuracy    : 45.8%    # Percentage of valid DOIs
```

This breakdown reveals that models may get titles right but fabricate author names or DOIs—a common pattern where the model "remembers" a paper's topic but not its exact metadata.

**Per-Discipline Performance** shows which academic fields are most challenging:

```
Per-Discipline Verification Rate:
  Computer Science : 81.2%
  Biomedical       : 74.5%
  Physics          : 70.3%
  Chemistry        : 65.8%
  Mathematics      : 58.1%
```


## Error Analysis

Analyze verification results to understand hallucination patterns and guide model selection:

### Verification Status Categories

Each reference receives one of four verification statuses:

| Status | Meaning | Typical Cause |
|--------|---------|---------------|
| **VERIFIED** | Reference confirmed as real | Paper found in academic databases with matching metadata |
| **SUSPECT** | Partial match found | Title similar but author/year mismatch; may be a real paper with wrong details |
| **NOT_FOUND** | No match in any database | Likely a fabricated reference |
| **ERROR** | Verification failed | API timeout, rate limiting, or network issues |

### Common Hallucination Patterns

| Pattern | Description | Detection |
|---------|-------------|-----------|
| **Plausible fabrication** | Paper sounds real but does not exist | High title similarity to real papers but no exact match |
| **Author swapping** | Correct paper title but wrong authors | Title verified but author accuracy low |
| **Year shifting** | Real paper but wrong publication year | Title/author match but year mismatch |
| **DOI invention** | Fabricated DOI that follows valid format | DOI format is correct but resolves to nothing |
| **Journal confusion** | Real paper attributed to wrong venue | Paper exists but published in different journal |

### Programmatic Error Analysis

```python
import json

# Load verification results
with open("evaluation_results/ref_hallucination_arena/verification_results.json") as f:
    results = json.load(f)

# Analyze hallucination patterns per model
for model_name, model_results in results.items():
    total = sum(r["total_refs"] for r in model_results)
    verified = sum(r["verified"] for r in model_results)
    not_found = sum(r["not_found"] for r in model_results)

    print(f"\n{model_name}:")
    print(f"  Total refs: {total}")
    print(f"  Verified: {verified} ({verified/total:.1%})")
    print(f"  Not found: {not_found} ({not_found/total:.1%})")

    # Per-discipline breakdown
    by_discipline = {}
    for r in model_results:
        d = r.get("discipline", "unknown")
        if d not in by_discipline:
            by_discipline[d] = {"total": 0, "verified": 0}
        by_discipline[d]["total"] += r["total_refs"]
        by_discipline[d]["verified"] += r["verified"]

    for d, stats in by_discipline.items():
        rate = stats["verified"] / stats["total"] if stats["total"] > 0 else 0
        print(f"  {d}: {rate:.1%} ({stats['verified']}/{stats['total']})")
```

### Improving Model Performance

Based on error analysis, consider these strategies:

| Error Pattern | Root Cause | Solution |
|---------------|------------|----------|
| Low verification rate overall | Model lacks factual grounding | Use models with retrieval augmentation (RAG) |
| High SUSPECT rate | Partial knowledge of papers | Strengthen system prompt to require exact metadata |
| Poor DOI accuracy | DOIs are hard to memorize | Ask models to omit DOIs if uncertain |
| Discipline-specific weakness | Domain knowledge gaps | Use domain-specialized models for specific fields |
| Year constraint violations | Model ignores temporal restrictions | Emphasize time constraints in the prompt |


## Output Files

All results are saved to the configured output directory:

```
evaluation_results/ref_hallucination_arena/
├── evaluation_report.md          # Detailed Markdown report
├── evaluation_results.json       # Final rankings and scores
├── verification_chart.png        # Verification rate bar chart
├── discipline_chart.png          # Per-discipline breakdown chart
├── queries.json                  # Loaded evaluation queries
├── responses.json                # Raw model responses
├── extracted_refs.json           # Extracted BibTeX references
├── verification_results.json     # Detailed verification results
└── checkpoint.json               # Pipeline checkpoint for resume
```


## Advanced Topics

=== "Verification Sources"

    The `CompositeVerifier` checks references against four academic databases in parallel:

    | Source | Coverage | Best For |
    |--------|----------|----------|
    | **Crossref** | Broadest coverage (130M+ records) | General academic papers with DOIs |
    | **PubMed** | Biomedical and life sciences | Medical, biological, and health papers |
    | **arXiv** | Preprints in STEM fields | Computer science, physics, mathematics |
    | **DBLP** | Computer science bibliography | CS conferences and journals |

    A reference is marked as VERIFIED if it matches in **any** of the four sources with a composite score above the configured threshold (default: 0.7).

    ??? tip "Increase Verification Rate Limits"
        Provide optional credentials to get higher API rate limits:

        ```yaml
        verification:
          crossref_mailto: "your-email@example.com"  # Join Crossref polite pool
          pubmed_api_key: "your-pubmed-api-key"       # Higher PubMed rate limit
        ```

=== "Checkpoint & Resume"

    Evaluations automatically save fine-grained checkpoints. Both response collection (Step 2) and reference verification (Step 4) support per-item checkpointing, so interrupted runs lose at most one item of progress:

    ```bash
    # First run (interrupted after verifying 500/1000 items)
    python -m cookbooks.ref_hallucination_arena --config config.yaml --save

    # Resume from checkpoint (automatically picks up at item 501)
    python -m cookbooks.ref_hallucination_arena --config config.yaml --save

    # Start fresh (ignore checkpoint)
    python -m cookbooks.ref_hallucination_arena --config config.yaml --fresh --save
    ```

    Checkpoint stages: `QUERIES_LOADED` → `RESPONSES_COLLECTING` → `RESPONSES_COLLECTED` → `REFS_EXTRACTED` → `VERIFICATION_IN_PROGRESS` → `VERIFICATION_COMPLETE` → `EVALUATION_COMPLETE`

=== "Custom System Prompts"

    The system prompt controls how models format their reference output. Use the `{num_refs}` placeholder to dynamically insert the expected number of references:

    ```yaml
    target_endpoints:
      my_model:
        base_url: "https://api.example.com/v1"
        api_key: "${API_KEY}"
        model: "my-model"
        system_prompt: |
          You are an academic literature recommendation expert.
          Based on the user's research topic, recommend {num_refs}
          real, high-quality academic papers. Output each paper in
          standard BibTeX format with title, author, year,
          journal/booktitle, and doi fields.
    ```

    !!! tip "BibTeX Format is Critical"
        The pipeline extracts references using BibTeX parsing. Ensure your system prompt explicitly requests BibTeX-formatted output for reliable extraction.

=== "Evaluation Report"

    Generate a comprehensive Markdown report with concrete examples:

    ```yaml
    report:
      enabled: true        # Enable report generation
      language: "zh"       # "zh" (Chinese) or "en" (English)
      include_examples: 3  # Examples per section (1-10)
      chart:
        enabled: true          # Generate visualization charts
        orientation: "vertical"  # "horizontal" or "vertical"
        show_values: true      # Show values on bars
        highlight_best: true   # Highlight best-performing model
    ```

    The report includes **Executive Summary**, **Model Rankings**, **Per-Discipline Analysis**, **Field-level Accuracy**, and **Representative Cases**.


## Best Practices

!!! tip "Do"
    - Use the **official dataset** from HuggingFace for reproducible and comparable results
    - Set `temperature: 0.3` or lower for more deterministic reference generation
    - Provide `crossref_mailto` to join the Crossref polite pool for better rate limits
    - Use `--save` flag to persist all intermediate results for later analysis
    - Include **diverse disciplines** in your evaluation queries for comprehensive assessment
    - Use `{num_refs}` placeholder in system prompts to control reference count

!!! warning "Don't"
    - Set `max_concurrency` too high—this may trigger API rate limits on verification services
    - Skip checkpoint resumption for large-scale evaluations (hundreds of queries × many models)
    - Compare models with different system prompts unless intentionally testing prompt effects
    - Ignore per-discipline results—aggregate scores can mask discipline-specific weaknesses


## Next Steps

- [Auto Arena](../applications/auto_arena.md) — Automatically compare models with generated queries
- [Refine Data Quality](../applications/data_refinement.md) — Improve model outputs using grader feedback
- [Create Custom Graders](../building_graders/create_custom_graders.md) — Build custom evaluation pipelines
